{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Implement Linear Regression and calculate sum of residual error on the following\n",
    "#        Datasets.\n",
    "#        x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "#        y = [1, 3, 2, 5, 7, 8, 8, 9, 10, 12]\n",
    "#        1.1) Compute the regression coefficients using analytic formulation and calculate Sum\n",
    "#        Squared Error (SSE) and R 2 value.\n",
    "#        1.2) Implement gradient descent (both Full-batch and Stochastic with stopping\n",
    "#        criteria) on Least Mean Square loss formulation to compute the coefficients of\n",
    "#        regression matrix and compare the results using performance measures such as R 2\n",
    "#        SSE etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE: 5.624242424242423\n",
      "R^2: 0.952538038613988\n"
     ]
    }
   ],
   "source": [
    "# 1.1) Compute the regression coefficients using analytic formulation and calculate Sum Squared Error (SSE) and R 2 value.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "\n",
    "mean_x = np.mean(x)\n",
    "mean_y = np.mean(y)\n",
    "\n",
    "numerator = np.sum((x - mean_x) * (y - mean_y))\n",
    "denominator = np.sum((x - mean_x) ** 2)\n",
    "slope = numerator / denominator\n",
    "intercept = mean_y - slope * mean_x\n",
    "\n",
    "y_pred = slope * x + intercept\n",
    "\n",
    "SSE = np.sum((y - y_pred) ** 2)\n",
    "\n",
    "SST = np.sum((y - mean_y) ** 2)\n",
    "R_squared = 1 - (SSE / SST)\n",
    "\n",
    "\n",
    "print(\"SSE:\", SSE)\n",
    "print(\"R^2:\", R_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-batch Gradient Descent:\n",
      "\n",
      "Coefficients: [1.17580361 1.17935476]\n",
      "SE: 5.634861529064238\n",
      "R-squared value: 0.9524484259150697\n",
      "\n",
      "Stochastic Gradient Descent:\n",
      "\n",
      "Coefficients: [1.43665929 1.43665929]\n",
      "SSE: 31.149481351391834\n",
      "R-squared value: 0.7371351784692672\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "\n",
    "\n",
    "def full_batch_gradient_descent(x, y, learning_rate, num_iterations):\n",
    "\n",
    "    theta = np.zeros(2)\n",
    "    m = len(y)\n",
    "    X = np.vstack((np.ones_like(x), x)).T\n",
    "    for _ in range(num_iterations):\n",
    "        y_pred = X.dot(theta)\n",
    "        theta -= (1/m) * learning_rate * X.T.dot(y_pred - y)\n",
    "    \n",
    "    return theta\n",
    "\n",
    "def stochastic_gradient_descent(x, y, learning_rate, num_iterations):\n",
    "    theta = np.zeros(2)\n",
    "    m = len(y)\n",
    "    for _ in range(num_iterations):\n",
    "        for i in range(m):\n",
    "            rand_index = np.random.randint(0, m)\n",
    "            xi = x[rand_index]\n",
    "            yi = y[rand_index]\n",
    "            y_pred = np.dot(xi, theta)\n",
    "            theta -= learning_rate * xi * (y_pred - yi)\n",
    "    \n",
    "    return theta\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "theta_full_batch = full_batch_gradient_descent(x, y, learning_rate, num_iterations)\n",
    "theta_stochastic = stochastic_gradient_descent(x, y, learning_rate, num_iterations)\n",
    "\n",
    "X = np.vstack((np.ones_like(x), x)).T\n",
    "\n",
    "y_pred_full_batch = X.dot(theta_full_batch)\n",
    "y_pred_stochastic = X.dot(theta_stochastic)\n",
    "\n",
    "SSE_full_batch = np.sum((y - y_pred_full_batch) ** 2)\n",
    "SSE_stochastic = np.sum((y - y_pred_stochastic) ** 2)\n",
    "\n",
    "mean_y = np.mean(y)\n",
    "SST = np.sum((y - mean_y) ** 2)\n",
    "R_squared_full_batch = 1 - (SSE_full_batch / SST)\n",
    "R_squared_stochastic = 1 - (SSE_stochastic / SST)\n",
    "\n",
    "# Step 8: Print results\n",
    "print(\"Full-batch Gradient Descent:\")\n",
    "print(\"\\nCoefficients:\", theta_full_batch)\n",
    "print(\"SE:\", SSE_full_batch)\n",
    "print(\"R-squared value:\", R_squared_full_batch)\n",
    "\n",
    "print(\"\\nStochastic Gradient Descent:\")\n",
    "print(\"\\nCoefficients:\", theta_stochastic)\n",
    "print(\"SSE:\", SSE_stochastic)\n",
    "print(\"R-squared value:\", R_squared_stochastic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
